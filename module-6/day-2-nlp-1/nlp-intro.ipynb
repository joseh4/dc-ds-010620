{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "<img src=\"img/text-miners.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora\n",
    " - sentence\n",
    " - word\n",
    " - corpus\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## start small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = \"Here is a sentence. Or two, I don't think there will be more.\"\n",
    "token_test_2 = \"i thought this sentence was good.\"\n",
    "token_test_3 = \"Here's a sentence... maybe two. Depending on how you like to count!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's a sentence  maybe two\", 'Depending on how you like to count!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document... into sentences\n",
    "def make_sentences(doc):\n",
    "    doc = doc.replace('...',' ')\n",
    "    sentences = doc.split('.')\n",
    "    return [s.strip() for s in sentences if s]\n",
    "\n",
    "make_sentences(token_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence.',\n",
       " 'Or',\n",
       " 'two,',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'think',\n",
       " 'there',\n",
       " 'will',\n",
       " 'be',\n",
       " 'more.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document into words\n",
    "# with these 3 test cases what would you look out for?\n",
    "def tokenize_it(doc):\n",
    "    return doc.split(' ')\n",
    "\n",
    "tokenize_it(token_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New library!\n",
    "\n",
    "while we have seen language processing tools in spark, NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
    "metamorph = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\n",
      "Translated by David Wyllie.\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\n",
      "**     Please follow the copyright guidelines in this file.     **\n",
      "\n",
      "\n",
      "Title: Metamorphosis\n",
      "\n",
      "Author: Franz Kafka\n",
      "\n",
      "Translator: David Wyllie\n",
      "\n",
      "Release Date: August 16, 2005 [EBook #5200]\n",
      "First posted: May 13, 2002\n",
      "Last updated: May 20, 2012\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright (C) 2002 David Wyllie.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Metamorphosis\n",
      "  Franz Kafka\n",
      "\n",
      "Translated by David Wyllie\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "print(metamorph[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:['-][a-z]+)?)\"\n",
    "metamorph_tokens_raw = nltk.regexp_tokenize(metamorph, pattern)\n",
    "print(metamorph_tokens_raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25098"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metamorph_tokens_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this', 'project']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.lower() for i in metamorph_tokens_raw]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this', 'project']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.replace(\"''\",' ').replace(\"-\",'') for i in metamorph_tokens]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joseph/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'www', 'gutenberg', 'net', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly']\n"
     ]
    }
   ],
   "source": [
    "# the set function gives an ordered list of unique elements\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "metamorph_tokens_stopped = [w for w in metamorph_tokens if not w in stop_words]\n",
    "print(metamorph_tokens_stopped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "<img src=\"https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress\n",
      "fli\n",
      "die\n",
      "mule\n",
      "deni\n",
      "die\n",
      "agre\n",
      "own\n",
      "humbl\n",
      "size\n",
      "meet\n",
      "state\n",
      "siez\n",
      "item\n",
      "sensat\n",
      "tradit\n",
      "refer\n",
      "colon\n",
      "plot\n"
     ]
    }
   ],
   "source": [
    "singles = [stemmer.stem(e) for e in example]\n",
    "print(*singles, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "<img src=\"https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "print(*SnowballStemmer.languages, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'reus', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'www', 'gutenberg', 'net', 'copyright', 'project', 'gutenberg', 'ebook', 'detail', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'ebook', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosi', 'copyright', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slight']\n"
     ]
    }
   ],
   "source": [
    "meta_stemmed = [stemmer.stem(word) for word in metamorph_tokens_stopped]\n",
    "print(meta_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/joseph/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joseph/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'NN'),\n",
       " ('gutenberg', 'NN'),\n",
       " ('ebook', 'NN'),\n",
       " ('metamorphosis', 'NN'),\n",
       " ('franz', 'NN'),\n",
       " ('kafka', 'NN'),\n",
       " ('translated', 'VBD'),\n",
       " ('david', 'JJ'),\n",
       " ('wyllie', 'NN'),\n",
       " ('ebook', 'NN')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(metamorph_tokens_stopped)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph_lemmas_pos = []\n",
    "for x, y in nltk.pos_tag(metamorph_tokens_stopped):\n",
    "    metamorph_lemmas_pos.append((x, get_wordnet_pos(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'n'),\n",
       " ('gutenberg', 'n'),\n",
       " ('ebook', 'n'),\n",
       " ('metamorphosis', 'n'),\n",
       " ('franz', 'n'),\n",
       " ('kafka', 'n'),\n",
       " ('translated', 'v'),\n",
       " ('david', 'a'),\n",
       " ('wyllie', 'n'),\n",
       " ('ebook', 'n')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_lemmas_pos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Lemmatizer on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('domed', 'domed')\n",
      "('divided', 'divided')\n",
      "('arches', 'arch')\n",
      "('stiff', 'stiff')\n",
      "('sections', 'section')\n",
      "('bedding', 'bed')\n",
      "('hardly', 'hardly')\n",
      "('able', 'able')\n",
      "('cover', 'cover')\n",
      "('seemed', 'seem')\n",
      "('ready', 'ready')\n",
      "('slide', 'slide')\n",
      "('moment', 'moment')\n",
      "('many', 'many')\n",
      "('legs', 'leg')\n",
      "('pitifully', 'pitifully')\n",
      "('thin', 'thin')\n",
      "('compared', 'compare')\n",
      "('size', 'size')\n",
      "('rest', 'rest')\n",
      "('waved', 'wave')\n",
      "('helplessly', 'helplessly')\n",
      "('looked', 'look')\n",
      "(\"what's\", \"what's\")\n",
      "('happened', 'happen')\n",
      "('thought', 'thought')\n",
      "('dream', 'dream')\n",
      "('room', 'room')\n",
      "('proper', 'proper')\n",
      "('human', 'human')\n",
      "('room', 'room')\n",
      "('although', 'although')\n",
      "('little', 'little')\n",
      "('small', 'small')\n",
      "('lay', 'lay')\n",
      "('peacefully', 'peacefully')\n",
      "('four', 'four')\n",
      "('familiar', 'familiar')\n",
      "('walls', 'wall')\n",
      "('collection', 'collection')\n",
      "('textile', 'textile')\n",
      "('samples', 'sample')\n",
      "('lay', 'lay')\n",
      "('spread', 'spread')\n",
      "('table', 'table')\n",
      "('samsa', 'samsa')\n",
      "('travelling', 'travel')\n",
      "('salesman', 'salesman')\n",
      "('hung', 'hung')\n",
      "('picture', 'picture')\n",
      "('recently', 'recently')\n",
      "('cut', 'cut')\n",
      "('illustrated', 'illustrated')\n",
      "('magazine', 'magazine')\n",
      "('housed', 'house')\n",
      "('nice', 'nice')\n",
      "('gilded', 'gild')\n",
      "('frame', 'frame')\n",
      "('showed', 'show')\n",
      "('lady', 'lady')\n",
      "('fitted', 'fit')\n",
      "('fur', 'fur')\n",
      "('hat', 'hat')\n",
      "('fur', 'fur')\n",
      "('boa', 'boa')\n",
      "('sat', 'sit')\n",
      "('upright', 'upright')\n",
      "('raising', 'raise')\n",
      "('heavy', 'heavy')\n",
      "('fur', 'fur')\n",
      "('muff', 'muff')\n",
      "('covered', 'cover')\n",
      "('whole', 'whole')\n",
      "('lower', 'low')\n",
      "('arm', 'arm')\n",
      "('towards', 'towards')\n",
      "('viewer', 'viewer')\n",
      "('gregor', 'gregor')\n",
      "('turned', 'turn')\n",
      "('look', 'look')\n",
      "('window', 'window')\n",
      "('dull', 'dull')\n",
      "('weather', 'weather')\n",
      "('drops', 'drop')\n",
      "('rain', 'rain')\n",
      "('could', 'could')\n",
      "('heard', 'hear')\n",
      "('hitting', 'hit')\n",
      "('pane', 'pane')\n",
      "('made', 'make')\n",
      "('feel', 'feel')\n",
      "('quite', 'quite')\n",
      "('sad', 'sad')\n",
      "('sleep', 'sleep')\n",
      "('little', 'little')\n",
      "('bit', 'bit')\n",
      "('longer', 'longer')\n",
      "('forget', 'forget')\n",
      "('nonsense', 'nonsense')\n",
      "('thought', 'thought')\n"
     ]
    }
   ],
   "source": [
    "meta_lemmaed = []\n",
    "for word, pos in metamorph_lemmas_pos:\n",
    "    meta_lemmaed.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "print(*zip(metamorph_tokens_stopped[100:200], meta_lemmaed[100:200]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_freqdist = FreqDist(meta_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gregor', 298),\n",
       " ('would', 187),\n",
       " ('room', 133),\n",
       " ('could', 120),\n",
       " ('work', 114),\n",
       " ('even', 104),\n",
       " ('father', 102),\n",
       " ('sister', 101),\n",
       " ('door', 97),\n",
       " ('mother', 90),\n",
       " ('project', 88),\n",
       " ('back', 83),\n",
       " ('time', 74),\n",
       " ('way', 66),\n",
       " ('one', 61),\n",
       " ('look', 61),\n",
       " ('gutenbergtm', 57),\n",
       " ('open', 56),\n",
       " ('use', 53),\n",
       " ('get', 52),\n",
       " ('said', 51),\n",
       " ('littl', 49),\n",
       " ('go', 49),\n",
       " ('without', 47),\n",
       " ('first', 45),\n",
       " ('still', 45),\n",
       " ('want', 44),\n",
       " ('see', 42),\n",
       " ('like', 41),\n",
       " ('hand', 41),\n",
       " ('made', 40),\n",
       " ('make', 40),\n",
       " ('head', 39),\n",
       " ('much', 39),\n",
       " ('come', 39),\n",
       " ('day', 38),\n",
       " ('thing', 38),\n",
       " ('move', 38),\n",
       " ('chief', 38),\n",
       " ('gutenberg', 37),\n",
       " ('thought', 37),\n",
       " ('clerk', 37),\n",
       " ('turn', 36),\n",
       " ('away', 35),\n",
       " ('samsa', 34),\n",
       " ('let', 33),\n",
       " ('bed', 32),\n",
       " ('went', 32),\n",
       " ('famili', 32),\n",
       " ('seem', 31)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129e8b810>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "meta_freqdist.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## this step happens after we account for stopwords and lemmas; depending on the library...\n",
    "* we make a **Count Vector**, which is the formal term for a **bag of words**\n",
    "* we use vectors to pass text into machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the CountVectorizer method on 'basic_example'\n",
    "basic_example = ['The Data Scientist wants to train a machine to train machine learning models.']\n",
    "cv = CountVectorizer()\n",
    "cv.fit(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what info can we get from cv?\n",
    "# hint -- look at the docs again\n",
    "cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization allows us to compare two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to help see what's happening\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the CountVectorizer on the 'basic_example', now we transform 'basic_example'\n",
    "example_vector_doc_1 = cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# what is the type\n",
    "\n",
    "print(type(example_vector_doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# what does it look like\n",
    "\n",
    "print(example_vector_doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         1        2       1          1    1   2      2      1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "example_vector_df = pd.DataFrame(example_vector_doc_1.toarray(), columns=cv.get_feature_names())\n",
    "example_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         0        0       0          1    2   0      0      0\n",
       "1     0         0        0       0          0    1   2      0      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compare new text to the CountVectorizer fit on 'basic_example'\n",
    "new_text = ['the data scientist plotted the residual error of her model',\n",
    "           'I want to go outside to enjoy the nice weather']\n",
    "new_data = cv.transform(new_text)\n",
    "new_count = pd.DataFrame(new_data.toarray(),columns=cv.get_feature_names())\n",
    "new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this the object 'sentences' becomes the corpus\n",
    "sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "             'the data scientist plotted the residual error of her model in her analysis',\n",
    "             'Her analysis was so good, she won a Kaggle competition.',\n",
    "             'The machine gained sentience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the docs for count vectorizer, how would we use an ngram\n",
    "# pro tip -- include stop words\n",
    "bigrams = CountVectorizer(stop_words='english', ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x18 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vector = bigrams.fit_transform(sentences)\n",
    "bigram_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 features for this corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analysis good',\n",
       " 'data scientist',\n",
       " 'error model',\n",
       " 'gained sentience',\n",
       " 'good won',\n",
       " 'kaggle competition',\n",
       " 'learning models',\n",
       " 'machine gained',\n",
       " 'machine learning',\n",
       " 'machine train']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {str(len(bigrams.get_feature_names()))} features for this corpus')\n",
    "bigrams.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis good</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error model</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good won</th>\n",
       "      <th>kaggle competition</th>\n",
       "      <th>learning models</th>\n",
       "      <th>machine gained</th>\n",
       "      <th>machine learning</th>\n",
       "      <th>machine train</th>\n",
       "      <th>model analysis</th>\n",
       "      <th>plotted residual</th>\n",
       "      <th>residual error</th>\n",
       "      <th>scientist plotted</th>\n",
       "      <th>scientist wants</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants train</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis good  data scientist  error model  gained sentience  good won  \\\n",
       "0              0               1            0                 0         0   \n",
       "1              0               1            1                 0         0   \n",
       "2              1               0            0                 0         1   \n",
       "3              0               0            0                 1         0   \n",
       "\n",
       "   kaggle competition  learning models  machine gained  machine learning  \\\n",
       "0                   0                1               0                 1   \n",
       "1                   0                0               0                 0   \n",
       "2                   1                0               0                 0   \n",
       "3                   0                0               1                 0   \n",
       "\n",
       "   machine train  model analysis  plotted residual  residual error  \\\n",
       "0              1               0                 0               0   \n",
       "1              0               1                 1               1   \n",
       "2              0               0                 0               0   \n",
       "3              0               0                 0               0   \n",
       "\n",
       "   scientist plotted  scientist wants  train machine  wants train  won kaggle  \n",
       "0                  0                1              2            1           0  \n",
       "1                  1                0              0            0           0  \n",
       "2                  0                0              0            0           1  \n",
       "3                  0                0              0            0           0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "bigram_df = pd.DataFrame(bigram_vector.toarray(), columns=bigrams.get_feature_names())\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "                    'the data scientist plotted the residual error of her model in her analysis',\n",
    "                    'Her analysis was so good, she won a Kaggle competition.',\n",
    "                    'The machine gained sentiance']\n",
    "# take out stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# fit transform the sentences\n",
    "tfidf_sentences = tfidf.fit_transform(tf_idf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "tfidf_df = pd.DataFrame(tfidf_sentences.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentiance</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610575</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition      data     error    gained      good    kaggle  \\\n",
       "0  0.000000     0.000000  0.240692  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.325557     0.000000  0.325557  0.412928  0.000000  0.000000  0.000000   \n",
       "2  0.366739     0.465162  0.000000  0.000000  0.000000  0.465162  0.465162   \n",
       "3  0.000000     0.000000  0.000000  0.000000  0.617614  0.000000  0.000000   \n",
       "\n",
       "   learning   machine     model    models   plotted  residual  scientist  \\\n",
       "0  0.305288  0.481384  0.000000  0.305288  0.000000  0.000000   0.240692   \n",
       "1  0.000000  0.000000  0.412928  0.000000  0.412928  0.412928   0.325557   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.486934  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   sentiance     train     wants       won  \n",
       "0   0.000000  0.610575  0.305288  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.465162  \n",
       "3   0.617614  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to bigrams\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out our TfidfVectorizer\n",
    "test_tdidf = tfidf.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a vector\n",
    "test_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_df = pd.DataFrame(test_tdidf.toarray(), columns=tfidf.get_feature_names())\n",
    "test_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same. A potential application of cosine similarity is a basic recommendation engine. If you wanted to recommend articles that are most similar to other articles, you could talk the cosine similarity of all articles and return the highest one.\n",
    "\n",
    "<img src=\"./img/better_cos_similarity.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "                    'I ate a hot dog at burger prince and it was bad',\n",
    "                    'I drove a racecar through your kitchen door',\n",
    "                    'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "trial.fit(sunday_afternoon)\n",
    "text_data = trial.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91413793]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[0],text_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
