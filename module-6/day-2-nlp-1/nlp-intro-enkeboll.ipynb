{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "<img src=\"img/text-miners.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora\n",
    " - sentence\n",
    " - word\n",
    " - corpus\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## start small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = \"Here is a sentence. Or two, I don't think there will be more.\"\n",
    "token_test_2 = \"i thought this sentence was good.\"\n",
    "token_test_3 = \"Here's a sentence... maybe two. Depending on how you like to count!\"\n",
    "token_test_4 = \"The best teacher in the world is Dr. Guner! He really knows his stuff.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's a sentence  maybe two\", 'Depending on how you like to count!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document... into sentences\n",
    "def make_sentences(doc):\n",
    "    doc = doc.replace('...', ' ')\n",
    "    sentences = doc.split('.')\n",
    "    return [s.strip() for s in sentences if s]\n",
    "\n",
    "make_sentences(token_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence.',\n",
       " 'Or',\n",
       " 'two,',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'think',\n",
       " 'there',\n",
       " 'will',\n",
       " 'be',\n",
       " 'more.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize a document into words\n",
    "# with these 3 test cases what would you look out for?\n",
    "def tokenize_it(doc):\n",
    "    return doc.split(' ')\n",
    "\n",
    "tokenize_it(token_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New library!\n",
    "\n",
    "while we have seen language processing tools in spark, NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
    "metamorph = resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\r\n",
      "Translated by David Wyllie.\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.net\r\n",
      "\r\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n",
      "**     Please follow the copyright guidelines in this file.     **\r\n",
      "\r\n",
      "\r\n",
      "Title: Metamorphosis\r\n",
      "\r\n",
      "Author: Franz Kafka\r\n",
      "\r\n",
      "Translator: David Wyllie\r\n",
      "\r\n",
      "Release Date: August 16, 2005 [EBook #5200]\r\n",
      "First posted: May 13, 2002\r\n",
      "Last updated: May 20, 2012\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Copyright (C) 2002 David Wyllie.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  Metamorphosis\r\n",
      "  Franz Kafka\r\n",
      "\r\n",
      "Translated by David Wyllie\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "\r\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "print(metamorph[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:['-][a-z]+)?)\"\n",
    "metamorph_tokens_raw = nltk.regexp_tokenize(metamorph, pattern)\n",
    "print(metamorph_tokens_raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25098"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metamorph_tokens_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this', 'project']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.lower() for i in metamorph_tokens_raw]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this', 'project']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.replace(\"'\", '').replace(\"-\", '') for i in metamorph_tokens]\n",
    "print(metamorph_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/enkeboll/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 7}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [5,5,5,5,4,5,7,7,5,2,2,1,3,1,0]\n",
    "set(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'www', 'gutenberg', 'net', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "metamorph_tokens_stopped = [w for w in metamorph_tokens if not w in stop_words]\n",
    "print(metamorph_tokens_stopped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "<img src=\"https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'agreeable', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress\n",
      "fli\n",
      "die\n",
      "mule\n",
      "deni\n",
      "die\n",
      "agre\n",
      "agreeabl\n",
      "own\n",
      "humbl\n",
      "size\n",
      "meet\n",
      "state\n",
      "siez\n",
      "item\n",
      "sensat\n",
      "tradit\n",
      "refer\n",
      "colon\n",
      "plot\n"
     ]
    }
   ],
   "source": [
    "singles = [stemmer.stem(e) for e in example]\n",
    "print(*singles, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "<img src=\"https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "print(*SnowballStemmer.languages, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('caress', 'caress')\n",
      "('fli', 'fli')\n",
      "('die', 'die')\n",
      "('mule', 'mule')\n",
      "('deni', 'deni')\n",
      "('die', 'die')\n",
      "('agre', 'agre')\n",
      "('agreeabl', 'agreeabl')\n",
      "('own', 'own')\n",
      "('humbl', 'humbl')\n",
      "('size', 'size')\n",
      "('meet', 'meet')\n",
      "('state', 'state')\n",
      "('siez', 'siez')\n",
      "('item', 'item')\n",
      "('sensat', 'sensat')\n",
      "('tradit', 'tradit')\n",
      "('refer', 'refer')\n",
      "('colon', 'colon')\n",
      "('plot', 'plot')\n"
     ]
    }
   ],
   "source": [
    "singles = [(porter.stem(e), snowball.stem(e)) for e in example]\n",
    "print(*singles, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'reus', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'www', 'gutenberg', 'net', 'copyright', 'project', 'gutenberg', 'ebook', 'detail', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'ebook', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosi', 'copyright', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slight']\n"
     ]
    }
   ],
   "source": [
    "meta_stemmed = [snowball.stem(word) for word in metamorph_tokens_stopped]\n",
    "print(meta_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'v', 'a', 'r']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import POS_LIST\n",
    "POS_LIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import ADV\n",
    "ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 'NN'),\n",
       " ('gutenberg', 'NN'),\n",
       " ('ebook', 'NN'),\n",
       " ('metamorphosis', 'NN'),\n",
       " ('franz', 'NN'),\n",
       " ('kafka', 'NN'),\n",
       " ('translated', 'VBD'),\n",
       " ('david', 'JJ'),\n",
       " ('wyllie', 'NN'),\n",
       " ('ebook', 'NN')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(metamorph_tokens_stopped)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph_lemmas_pos = []\n",
    "for x, y in nltk.pos_tag(metamorph_tokens_stopped):\n",
    "    metamorph_lemmas_pos.append((x, get_wordnet_pos(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domed', 'v'),\n",
       " ('divided', 'a'),\n",
       " ('arches', 'n'),\n",
       " ('stiff', 'a'),\n",
       " ('sections', 'n'),\n",
       " ('bedding', 'v'),\n",
       " ('hardly', 'r'),\n",
       " ('able', 'a'),\n",
       " ('cover', 'n'),\n",
       " ('seemed', 'v'),\n",
       " ('ready', 'a'),\n",
       " ('slide', 'a'),\n",
       " ('moment', 'n'),\n",
       " ('many', 'a'),\n",
       " ('legs', 'n'),\n",
       " ('pitifully', 'r'),\n",
       " ('thin', 'a'),\n",
       " ('compared', 'v'),\n",
       " ('size', 'n'),\n",
       " ('rest', 'n'),\n",
       " ('waved', 'v'),\n",
       " ('helplessly', 'r'),\n",
       " ('looked', 'v'),\n",
       " ('whats', 'n'),\n",
       " ('happened', 'v'),\n",
       " ('thought', 'v'),\n",
       " ('wasnt', 'a'),\n",
       " ('dream', 'n'),\n",
       " ('room', 'n'),\n",
       " ('proper', 'n'),\n",
       " ('human', 'a'),\n",
       " ('room', 'n'),\n",
       " ('although', 'n'),\n",
       " ('little', 'a'),\n",
       " ('small', 'a'),\n",
       " ('lay', 'v'),\n",
       " ('peacefully', 'r'),\n",
       " ('four', 'n'),\n",
       " ('familiar', 'a'),\n",
       " ('walls', 'n'),\n",
       " ('collection', 'n'),\n",
       " ('textile', 'n'),\n",
       " ('samples', 'n'),\n",
       " ('lay', 'v'),\n",
       " ('spread', 'n'),\n",
       " ('table', 'n'),\n",
       " ('samsa', 'n'),\n",
       " ('travelling', 'v'),\n",
       " ('salesman', 'a'),\n",
       " ('hung', 'a'),\n",
       " ('picture', 'n'),\n",
       " ('recently', 'r'),\n",
       " ('cut', 'v'),\n",
       " ('illustrated', 'a'),\n",
       " ('magazine', 'n'),\n",
       " ('housed', 'v'),\n",
       " ('nice', 'r'),\n",
       " ('gilded', 'v'),\n",
       " ('frame', 'n'),\n",
       " ('showed', 'v'),\n",
       " ('lady', 'a'),\n",
       " ('fitted', 'v'),\n",
       " ('fur', 'n'),\n",
       " ('hat', 'n'),\n",
       " ('fur', 'v'),\n",
       " ('boa', 'n'),\n",
       " ('sat', 'v'),\n",
       " ('upright', 'a'),\n",
       " ('raising', 'v'),\n",
       " ('heavy', 'a'),\n",
       " ('fur', 'a'),\n",
       " ('muff', 'n'),\n",
       " ('covered', 'v'),\n",
       " ('whole', 'a'),\n",
       " ('lower', 'a'),\n",
       " ('arm', 'n'),\n",
       " ('towards', 'n'),\n",
       " ('viewer', 'v'),\n",
       " ('gregor', 'n'),\n",
       " ('turned', 'v'),\n",
       " ('look', 'n'),\n",
       " ('window', 'n'),\n",
       " ('dull', 'n'),\n",
       " ('weather', 'n'),\n",
       " ('drops', 'n'),\n",
       " ('rain', 'n'),\n",
       " ('could', 'n'),\n",
       " ('heard', 'v'),\n",
       " ('hitting', 'v'),\n",
       " ('pane', 'n'),\n",
       " ('made', 'v'),\n",
       " ('feel', 'n'),\n",
       " ('quite', 'r'),\n",
       " ('sad', 'a'),\n",
       " ('sleep', 'a'),\n",
       " ('little', 'a'),\n",
       " ('bit', 'n'),\n",
       " ('longer', 'r'),\n",
       " ('forget', 'v'),\n",
       " ('nonsense', 'a')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamorph_lemmas_pos[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helplessly'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('helplessly', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Lemmatizer on Metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('domed', 'domed')\n",
      "('divided', 'divided')\n",
      "('arches', 'arch')\n",
      "('stiff', 'stiff')\n",
      "('sections', 'section')\n",
      "('bedding', 'bed')\n",
      "('hardly', 'hardly')\n",
      "('able', 'able')\n",
      "('cover', 'cover')\n",
      "('seemed', 'seem')\n",
      "('ready', 'ready')\n",
      "('slide', 'slide')\n",
      "('moment', 'moment')\n",
      "('many', 'many')\n",
      "('legs', 'leg')\n",
      "('pitifully', 'pitifully')\n",
      "('thin', 'thin')\n",
      "('compared', 'compare')\n",
      "('size', 'size')\n",
      "('rest', 'rest')\n",
      "('waved', 'wave')\n",
      "('helplessly', 'helplessly')\n",
      "('looked', 'look')\n",
      "('whats', 'whats')\n",
      "('happened', 'happen')\n",
      "('thought', 'think')\n",
      "('wasnt', 'wasnt')\n",
      "('dream', 'dream')\n",
      "('room', 'room')\n",
      "('proper', 'proper')\n",
      "('human', 'human')\n",
      "('room', 'room')\n",
      "('although', 'although')\n",
      "('little', 'little')\n",
      "('small', 'small')\n",
      "('lay', 'lay')\n",
      "('peacefully', 'peacefully')\n",
      "('four', 'four')\n",
      "('familiar', 'familiar')\n",
      "('walls', 'wall')\n",
      "('collection', 'collection')\n",
      "('textile', 'textile')\n",
      "('samples', 'sample')\n",
      "('lay', 'lay')\n",
      "('spread', 'spread')\n",
      "('table', 'table')\n",
      "('samsa', 'samsa')\n",
      "('travelling', 'travel')\n",
      "('salesman', 'salesman')\n",
      "('hung', 'hung')\n",
      "('picture', 'picture')\n",
      "('recently', 'recently')\n",
      "('cut', 'cut')\n",
      "('illustrated', 'illustrated')\n",
      "('magazine', 'magazine')\n",
      "('housed', 'house')\n",
      "('nice', 'nice')\n",
      "('gilded', 'gild')\n",
      "('frame', 'frame')\n",
      "('showed', 'show')\n",
      "('lady', 'lady')\n",
      "('fitted', 'fit')\n",
      "('fur', 'fur')\n",
      "('hat', 'hat')\n",
      "('fur', 'fur')\n",
      "('boa', 'boa')\n",
      "('sat', 'sit')\n",
      "('upright', 'upright')\n",
      "('raising', 'raise')\n",
      "('heavy', 'heavy')\n",
      "('fur', 'fur')\n",
      "('muff', 'muff')\n",
      "('covered', 'cover')\n",
      "('whole', 'whole')\n",
      "('lower', 'low')\n",
      "('arm', 'arm')\n",
      "('towards', 'towards')\n",
      "('viewer', 'viewer')\n",
      "('gregor', 'gregor')\n",
      "('turned', 'turn')\n",
      "('look', 'look')\n",
      "('window', 'window')\n",
      "('dull', 'dull')\n",
      "('weather', 'weather')\n",
      "('drops', 'drop')\n",
      "('rain', 'rain')\n",
      "('could', 'could')\n",
      "('heard', 'hear')\n",
      "('hitting', 'hit')\n",
      "('pane', 'pane')\n",
      "('made', 'make')\n",
      "('feel', 'feel')\n",
      "('quite', 'quite')\n",
      "('sad', 'sad')\n",
      "('sleep', 'sleep')\n",
      "('little', 'little')\n",
      "('bit', 'bit')\n",
      "('longer', 'longer')\n",
      "('forget', 'forget')\n",
      "('nonsense', 'nonsense')\n"
     ]
    }
   ],
   "source": [
    "meta_lemmaed = []\n",
    "for word, pos in metamorph_lemmas_pos:\n",
    "    meta_lemmaed.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "print(*zip(metamorph_tokens_stopped[100:200], meta_lemmaed[100:200]), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_freqdist = FreqDist(meta_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gregor', 298),\n",
       " ('would', 187),\n",
       " ('room', 133),\n",
       " ('could', 120),\n",
       " ('work', 114),\n",
       " ('even', 104),\n",
       " ('father', 102),\n",
       " ('sister', 101),\n",
       " ('door', 97),\n",
       " ('mother', 90),\n",
       " ('project', 88),\n",
       " ('back', 83),\n",
       " ('time', 74),\n",
       " ('way', 66),\n",
       " ('one', 61),\n",
       " ('look', 61),\n",
       " ('gutenbergtm', 57),\n",
       " ('open', 56),\n",
       " ('use', 53),\n",
       " ('get', 52),\n",
       " ('said', 51),\n",
       " ('littl', 49),\n",
       " ('go', 49),\n",
       " ('without', 47),\n",
       " ('first', 45),\n",
       " ('still', 45),\n",
       " ('want', 44),\n",
       " ('see', 42),\n",
       " ('like', 41),\n",
       " ('hand', 41),\n",
       " ('made', 40),\n",
       " ('make', 40),\n",
       " ('head', 39),\n",
       " ('much', 39),\n",
       " ('come', 39),\n",
       " ('day', 38),\n",
       " ('thing', 38),\n",
       " ('move', 38),\n",
       " ('chief', 38),\n",
       " ('gutenberg', 37),\n",
       " ('thought', 37),\n",
       " ('clerk', 37),\n",
       " ('turn', 36),\n",
       " ('away', 35),\n",
       " ('samsa', 34),\n",
       " ('let', 33),\n",
       " ('bed', 32),\n",
       " ('well', 32),\n",
       " ('went', 32),\n",
       " ('famili', 32)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x132e61650>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "meta_freqdist.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## this step happens after we account for stopwords and lemmas; depending on the library...\n",
    "* we make a **Count Vector**, which is the formal term for a **bag of words**\n",
    "* we use vectors to pass text into machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the CountVectorizer method on 'basic_example'\n",
    "basic_example = ['The Data Scientist wants to train a machine to train machine learning models.']\n",
    "cv = CountVectorizer()\n",
    "cv.fit(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what info can we get from cv?\n",
    "# hint -- look at the docs again\n",
    "cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cv.transform(basic_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 1, 1, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'models',\n",
       " 'scientist',\n",
       " 'the',\n",
       " 'to',\n",
       " 'train',\n",
       " 'wants']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization allows us to compare two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to help see what's happening\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the CountVectorizer on the 'basic_example', now we transform 'basic_example'\n",
    "example_vector_doc_1 = cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# what is the type\n",
    "\n",
    "print(type(example_vector_doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does it look like\n",
    "\n",
    "print(example_vector_doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         1        2       1          1    1   2      2      1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "example_vector_df = pd.DataFrame(example_vector_doc_1.toarray(), columns=cv.get_feature_names())\n",
    "example_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         0        0       0          1    2   0      0      0\n",
       "1     0         0        0       0          0    1   0      0      0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compare new text to the CountVectorizer fit on 'basic_example'\n",
    "new_text = ['the data scientist plotted the residual error of her model',\n",
    "            'Dang, the bacon and eggs I just ate were delicious']\n",
    "new_data = cv.transform(new_text)\n",
    "new_count = pd.DataFrame(new_data.toarray(),columns=cv.get_feature_names())\n",
    "new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t2\n",
      "  (1, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_count.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this the object 'sentences' becomes the corpus\n",
    "sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "             'the data scientist plotted the residual error of her model in her analysis',\n",
    "             'Her analysis was so good, she won a Kaggle competition.',\n",
    "             'The machine gained sentience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the docs for count vectorizer, how would we use an ngram\n",
    "# pro tip -- include stop words\n",
    "bigrams = CountVectorizer(stop_words='english', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 2 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      "  1 0 0 1 1 0 2 2 1 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
      "  1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vector = bigrams.fit_transform(sentences)\n",
    "print(bigram_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 52 features for this corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'analysis good',\n",
       " 'analysis good won',\n",
       " 'competition',\n",
       " 'data',\n",
       " 'data scientist',\n",
       " 'data scientist plotted',\n",
       " 'data scientist wants',\n",
       " 'error',\n",
       " 'error model']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {str(len(bigrams.get_feature_names()))} features for this corpus')\n",
    "bigrams.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis good</th>\n",
       "      <th>analysis good won</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>data scientist plotted</th>\n",
       "      <th>data scientist wants</th>\n",
       "      <th>error</th>\n",
       "      <th>error model</th>\n",
       "      <th>...</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>train machine learning</th>\n",
       "      <th>train machine train</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants train</th>\n",
       "      <th>wants train machine</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "      <th>won kaggle competition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis good  analysis good won  competition  data  \\\n",
       "0         0              0                  0            0     1   \n",
       "1         1              0                  0            0     1   \n",
       "2         1              1                  1            1     0   \n",
       "3         0              0                  0            0     0   \n",
       "\n",
       "   data scientist  data scientist plotted  data scientist wants  error  \\\n",
       "0               1                       0                     1      0   \n",
       "1               1                       1                     0      1   \n",
       "2               0                       0                     0      0   \n",
       "3               0                       0                     0      0   \n",
       "\n",
       "   error model  ...  train  train machine  train machine learning  \\\n",
       "0            0  ...      2              2                       1   \n",
       "1            1  ...      0              0                       0   \n",
       "2            0  ...      0              0                       0   \n",
       "3            0  ...      0              0                       0   \n",
       "\n",
       "   train machine train  wants  wants train  wants train machine  won  \\\n",
       "0                    1      1            1                    1    0   \n",
       "1                    0      0            0                    0    0   \n",
       "2                    0      0            0                    0    1   \n",
       "3                    0      0            0                    0    0   \n",
       "\n",
       "   won kaggle  won kaggle competition  \n",
       "0           0                       0  \n",
       "1           0                       0  \n",
       "2           1                       1  \n",
       "3           0                       0  \n",
       "\n",
       "[4 rows x 52 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "bigram_df = pd.DataFrame(bigram_vector.toarray(), columns=bigrams.get_feature_names())\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "                    'the data scientist plotted the residual error of her model in her analysis',\n",
    "                    'Her analysis was so good, she won a Kaggle competition.',\n",
    "                    'The machine gained sentience']\n",
    "# take out stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "# fit transform the sentences\n",
    "tfidf_sentences = tfidf.fit_transform(tf_idf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "tfidf_df = pd.DataFrame(tfidf_sentences.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis good</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error</th>\n",
       "      <th>error model</th>\n",
       "      <th>gained</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>scientist</th>\n",
       "      <th>scientist plotted</th>\n",
       "      <th>scientist wants</th>\n",
       "      <th>sentience</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants train</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.174767</td>\n",
       "      <td>0.174767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443339</td>\n",
       "      <td>0.443339</td>\n",
       "      <td>0.221669</td>\n",
       "      <td>0.221669</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.295059</td>\n",
       "      <td>0.295059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232628</td>\n",
       "      <td>0.295059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.268509</td>\n",
       "      <td>0.34057</td>\n",
       "      <td>0.34057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34057</td>\n",
       "      <td>0.34057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis good  competition      data  data scientist     error  \\\n",
       "0  0.000000        0.00000      0.00000  0.174767        0.174767  0.000000   \n",
       "1  0.232628        0.00000      0.00000  0.232628        0.232628  0.295059   \n",
       "2  0.268509        0.34057      0.34057  0.000000        0.000000  0.000000   \n",
       "3  0.000000        0.00000      0.00000  0.000000        0.000000  0.000000   \n",
       "\n",
       "   error model    gained  gained sentience     good  ...  scientist  \\\n",
       "0     0.000000  0.000000          0.000000  0.00000  ...   0.174767   \n",
       "1     0.295059  0.000000          0.000000  0.00000  ...   0.232628   \n",
       "2     0.000000  0.000000          0.000000  0.34057  ...   0.000000   \n",
       "3     0.000000  0.465162          0.465162  0.00000  ...   0.000000   \n",
       "\n",
       "   scientist plotted  scientist wants  sentience     train  train machine  \\\n",
       "0           0.000000         0.221669   0.000000  0.443339       0.443339   \n",
       "1           0.295059         0.000000   0.000000  0.000000       0.000000   \n",
       "2           0.000000         0.000000   0.000000  0.000000       0.000000   \n",
       "3           0.000000         0.000000   0.465162  0.000000       0.000000   \n",
       "\n",
       "      wants  wants train      won  won kaggle  \n",
       "0  0.221669     0.221669  0.00000     0.00000  \n",
       "1  0.000000     0.000000  0.00000     0.00000  \n",
       "2  0.000000     0.000000  0.34057     0.34057  \n",
       "3  0.000000     0.000000  0.00000     0.00000  \n",
       "\n",
       "[4 rows x 36 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis good</th>\n",
       "      <th>analysis good won</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>data scientist plotted</th>\n",
       "      <th>data scientist wants</th>\n",
       "      <th>error</th>\n",
       "      <th>error model</th>\n",
       "      <th>...</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>train machine learning</th>\n",
       "      <th>train machine train</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants train</th>\n",
       "      <th>wants train machine</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "      <th>won kaggle competition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis good  analysis good won  competition  data  \\\n",
       "0         0              0                  0            0     1   \n",
       "1         1              0                  0            0     1   \n",
       "2         1              1                  1            1     0   \n",
       "3         0              0                  0            0     0   \n",
       "\n",
       "   data scientist  data scientist plotted  data scientist wants  error  \\\n",
       "0               1                       0                     1      0   \n",
       "1               1                       1                     0      1   \n",
       "2               0                       0                     0      0   \n",
       "3               0                       0                     0      0   \n",
       "\n",
       "   error model  ...  train  train machine  train machine learning  \\\n",
       "0            0  ...      2              2                       1   \n",
       "1            1  ...      0              0                       0   \n",
       "2            0  ...      0              0                       0   \n",
       "3            0  ...      0              0                       0   \n",
       "\n",
       "   train machine train  wants  wants train  wants train machine  won  \\\n",
       "0                    1      1            1                    1    0   \n",
       "1                    0      0            0                    0    0   \n",
       "2                    0      0            0                    0    1   \n",
       "3                    0      0            0                    0    0   \n",
       "\n",
       "   won kaggle  won kaggle competition  \n",
       "0           0                       0  \n",
       "1           0                       0  \n",
       "2           1                       1  \n",
       "3           0                       0  \n",
       "\n",
       "[4 rows x 52 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to bigrams\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out our TfidfVectorizer\n",
    "test_tdidf = tfidf.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x36 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a vector\n",
    "test_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analysis good</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>data scientist</th>\n",
       "      <th>error</th>\n",
       "      <th>error model</th>\n",
       "      <th>gained</th>\n",
       "      <th>gained sentience</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>scientist</th>\n",
       "      <th>scientist plotted</th>\n",
       "      <th>scientist wants</th>\n",
       "      <th>sentience</th>\n",
       "      <th>train</th>\n",
       "      <th>train machine</th>\n",
       "      <th>wants</th>\n",
       "      <th>wants train</th>\n",
       "      <th>won</th>\n",
       "      <th>won kaggle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analysis good  competition  data  data scientist  error  \\\n",
       "0       0.0            0.0          0.0   0.0             0.0    0.0   \n",
       "1       0.0            0.0          0.0   0.0             0.0    0.0   \n",
       "\n",
       "   error model  gained  gained sentience  good  ...  scientist  \\\n",
       "0          0.0     0.0               0.0   0.0  ...        0.0   \n",
       "1          0.0     0.0               0.0   0.0  ...        0.0   \n",
       "\n",
       "   scientist plotted  scientist wants  sentience  train  train machine  wants  \\\n",
       "0                0.0              0.0        0.0    0.0            0.0    0.0   \n",
       "1                0.0              0.0        0.0    0.0            0.0    0.0   \n",
       "\n",
       "   wants train  won  won kaggle  \n",
       "0          0.0  0.0         0.0  \n",
       "1          0.0  0.0         0.0  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_df = pd.DataFrame(test_tdidf.toarray(), columns=tfidf.get_feature_names())\n",
    "test_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same. A potential application of cosine similarity is a basic recommendation engine. If you wanted to recommend articles that are most similar to other articles, you could talk the cosine similarity of all articles and return the highest one.\n",
    "\n",
    "<img src=\"./img/better_cos_similarity.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "                    'I ate a hot dog at burger prince and it was bad',\n",
    "                    'I drove a racecar through your kitchen door',\n",
    "                    'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "trial.fit(sunday_afternoon)\n",
    "text_data = trial.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 1, 3, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[3].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91413793]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[0],text_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.to_numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1219667]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_df.to_numpy()[0].reshape(1, -1), tfidf_df.to_numpy()[1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
