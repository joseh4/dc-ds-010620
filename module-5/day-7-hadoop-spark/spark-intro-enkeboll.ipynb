{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Machine learning \n",
    "\n",
    "### Learning goals:\n",
    "- align the relationships between Hadoop, Spark\n",
    "- differentiate between Spark RDDs and Spark Dataframes and when each is appropriate\n",
    "- locate and explore the Spark.ML documentation\n",
    "- code along a text classification problem using four different ml algorithms, a data prep pipeline, and gridsearch to fine tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark context and concepts review\n",
    "<img src=\"https://images.pexels.com/photos/285173/pexels-photo-285173.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The story of Spark (in diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Start with Hadoop\n",
    "<img alt=\"diagram of hadoop v1 compared to hadoop v2\" src=\"img/yarn.png\" width=800>\n",
    "\n",
    "[diagram source](https://sites.google.com/site/codingbughunter/hadoop/yarn-general-discribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Yarn facilitates the resource allocation between Spark and the HDFS\n",
    "### YARN = Yet Another Resource Negotiator\n",
    "#### YARN is a subproduct of Hadoop\n",
    "![yarn diagram with spark](http://hortonworks.com/wp-content/uploads/2013/06/YARN.png)\n",
    "\n",
    "[diagram source](https://sites.google.com/site/codingbughunter/hadoop/yarn-general-discribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Then visualize the Spark ecosystem built on top of that\n",
    "\n",
    "<img alt=\"diagram of spark eco system components\" src=\"img/spark_eco.png\" width=600>\n",
    "\n",
    "[image source here](https://databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The story of Spark (a timeline)\n",
    "\n",
    "|<p align=\"left justify\">Date</p>|<p align=\"left justify\">Product</p>|<p align=\"left justify\">Update</p>|\n",
    "|:----|:-----|:-----|\n",
    "| 2002 | Hadoop | <p align=\"left justify\">Doug Cutting starts `Apache Nutch` researching sort/merge processing</p> |\n",
    "| 2006 | Hadoop |  <p align=\"left justify\">Leaves `Nutch` and joins `Yahoo`, renaming the project `Hadoop` </p>|\n",
    "| 2008 | Hadoop |  <p align=\"left justify\">`Hadoop` was made `Apache’s` top level project </p> |\n",
    "| Jan 2008 | Hadoop |  <p align=\"left justify\">v 0.10.1 released </p>|\n",
    "| 2009 | Spark | <p align=\"left justify\">started as a research project at the UC Berkeley AMPLab  </p>|\n",
    "| 2010 | Spark |  <p align=\"left justify\">open sourced </p>|\n",
    "| Sept 2012 | Spark |  <p align=\"left justify\">0.6.0 released </p>|\n",
    "| 2013 | Spark |  <p align=\"left justify\">moved to the `Apache` Software Foundation </p>|\n",
    "| Feb 2013| Spark |  <p align=\"left justify\">Spark 0.7 adds a Python API called `PySpark` </p>|\n",
    "| Sept 2013 | Spark | <p align=\"left justify\">0.8.0 introduces `MLlib` </p>|\n",
    "| 2013 | Databricks |  <p align=\"left justify\">Original Spark research team at UC Berkeley found Databricks</p> |\n",
    "| May 2014 |Spark |  <p align=\"left justify\">v 1.0 introduces Spark SQL, for loading and manipulating structured data in Spark</p>|\n",
    "|Mar 2015 | Spark | <p align=\"left justify\"> v 1.3.0 brings a new DataFrame API</p> |\n",
    "| Jan 2016|  Spark | <p align=\"left justify\"> v 1.6.0 brings a new Dataset API </p> |\n",
    "| Jul 2016 | Spark | <p align=\"left justify\"> v 2.0.0 In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R DataFrame is the main programming interface. <br> - SparkSession: new entry point that replaces the old SQLContext<br>- Native CSV data source, based on Databricks’ spark-csv module<br>- MLlib - The DataFrame-based API is now the primary API. The RDD-based API is entering maintenance mode </p> |\n",
    "| Jul 2017| Spark | <p align=\"left justify\"> v 2.2.0 drops support for Python 2.6 |\n",
    "| Nov 2018 | Spark | <p align=\"left justify\"> v 2.4.0<br> - This release adds Barrier Execution Mode for better integration with deep learning frameworks<br> - more integration between pandas UDF and spark DataFrames </p>|\n",
    "| Jan 2020| PySpark | <p align=\"left justify\"> Python 2 will be officially unsupported |\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark data objects\n",
    "\n",
    "<img alt=\"diagram of definitions of Spark objects from databricks\" src=\"https://databricks.com/wp-content/uploads/2018/05/rdd-1024x595.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differences between objects:\n",
    "<img alt=\"memory usage\" src=\"https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"type safety\" src=\"https://databricks.com/wp-content/uploads/2016/07/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use an RDD when:\n",
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a dataframe when:\n",
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "\n",
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n",
    "\n",
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review:\n",
    "\n",
    "- You are grabbing live tweets about the CW show 'Jane the Virgin' for later analysis. In the Spark ecosystem, where should you store them? an RDD or a DataFrame?\n",
    "\n",
    "- You have an RDD of data that you wish to use to build a predictive model. Should you leave it as an RDD or transform it to a DataFrame?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d6197d3264fc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fabcafe0c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|24|M|technician|85711\r\n",
      "2|53|F|other|94043\r\n",
      "3|23|M|writer|32067\r\n",
      "4|24|M|technician|43537\r\n",
      "5|33|F|other|15213\r\n",
      "6|42|M|executive|98101\r\n",
      "7|57|M|administrator|91344\r\n",
      "8|36|M|administrator|05201\r\n",
      "9|29|M|student|01002\r\n",
      "10|53|M|lawyer|90703\r\n"
     ]
    }
   ],
   "source": [
    "!head data/users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p_df = pd.read_csv('data/users.csv', sep='|', header=None, names=['id',\n",
    "                                                                  'age',\n",
    "                                                                  'gender',\n",
    "                                                                  'role',\n",
    "                                                                  'zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>role</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>939</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>student</td>\n",
       "      <td>33319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>940</td>\n",
       "      <td>32</td>\n",
       "      <td>M</td>\n",
       "      <td>administrator</td>\n",
       "      <td>02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>941</td>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>97229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>942</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>librarian</td>\n",
       "      <td>78209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>943</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>student</td>\n",
       "      <td>77841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  age gender           role    zip\n",
       "0      1   24      M     technician  85711\n",
       "1      2   53      F          other  94043\n",
       "2      3   23      M         writer  32067\n",
       "3      4   24      M     technician  43537\n",
       "4      5   33      F          other  15213\n",
       "..   ...  ...    ...            ...    ...\n",
       "938  939   26      F        student  33319\n",
       "939  940   32      M  administrator  02215\n",
       "940  941   20      M        student  97229\n",
       "941  942   48      F      librarian  78209\n",
       "942  943   22      M        student  77841\n",
       "\n",
       "[943 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/users.csv\").map(lambda line: line.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'],\n",
       " ['2', '53', 'F', 'other', '94043'],\n",
       " ['3', '23', 'M', 'writer', '32067'],\n",
       " ['4', '24', 'M', 'technician', '43537'],\n",
       " ['5', '33', 'F', 'other', '15213']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/users.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively:\n",
    "from_pandas_df = spark.createDataFrame(p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, age: bigint, gender: string, role: string, zip: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='1', _c1='24', _c2='M', _c3='technician', _c4='85711'),\n",
       " Row(_c0='2', _c1='53', _c2='F', _c3='other', _c4='94043'),\n",
       " Row(_c0='3', _c1='23', _c2='M', _c3='writer', _c4='32067'),\n",
       " Row(_c0='4', _c1='24', _c2='M', _c3='technician', _c4='43537'),\n",
       " Row(_c0='5', _c1='33', _c2='F', _c3='other', _c4='15213')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, age=24, gender='M', role='technician', zip='85711'),\n",
       " Row(id=2, age=53, gender='F', role='other', zip='94043'),\n",
       " Row(id=3, age=23, gender='M', role='writer', zip='32067'),\n",
       " Row(id=4, age=24, gender='M', role='technician', zip='43537'),\n",
       " Row(id=5, age=33, gender='F', role='other', zip='15213')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_pandas_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's already a DF, but this is the easy way to rename columns\n",
    "df = (spark.read.csv(\"data/users.csv\", sep=\"|\")\n",
    "           .toDF(\"id\", \"age\", \"gender\", \"occupation\", \"zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, age: string, gender: string, occupation: string, zip: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    df.where(\"occupation != 'other'\")\n",
    "      .groupby(\"occupation\")\n",
    "      .count()\n",
    "      .sort(\"count\", ascending=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>student</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>educator</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>administrator</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>engineer</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>programmer</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>librarian</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>writer</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>executive</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>scientist</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artist</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>technician</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>marketing</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>retired</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>salesman</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>none</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doctor</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       occupation  count\n",
       "0         student    196\n",
       "1        educator     95\n",
       "2   administrator     79\n",
       "3        engineer     67\n",
       "4      programmer     66\n",
       "5       librarian     51\n",
       "6          writer     45\n",
       "7       executive     32\n",
       "8       scientist     31\n",
       "9          artist     28\n",
       "10     technician     27\n",
       "11      marketing     26\n",
       "12  entertainment     18\n",
       "13     healthcare     16\n",
       "14        retired     14\n",
       "15         lawyer     12\n",
       "16       salesman     12\n",
       "17           none      9\n",
       "18      homemaker      7\n",
       "19         doctor      7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, age: string, gender: string, occupation: string, zip: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-----+\n",
      "| id|age|gender|occupation|  zip|\n",
      "+---+---+------+----------+-----+\n",
      "|  1| 24|     M|technician|85711|\n",
      "|  2| 53|     F|     other|94043|\n",
      "|  3| 23|     M|    writer|32067|\n",
      "|  4| 24|     M|technician|43537|\n",
      "|  5| 33|     F|     other|15213|\n",
      "+---+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(df.limit(5))\n",
    "# df.take(5).toPandas()\n",
    "type(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.agg(F.countDistinct('occupation')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------------+-----+-------------+-----+\n",
      "| id|age|gender|   occupation|  zip|   occupation|count|\n",
      "+---+---+------+-------------+-----+-------------+-----+\n",
      "|  1| 24|     M|   technician|85711|   technician|   27|\n",
      "|  3| 23|     M|       writer|32067|       writer|   45|\n",
      "|  4| 24|     M|   technician|43537|   technician|   27|\n",
      "|  6| 42|     M|    executive|98101|    executive|   32|\n",
      "|  7| 57|     M|administrator|91344|administrator|   79|\n",
      "|  8| 36|     M|administrator|05201|administrator|   79|\n",
      "|  9| 29|     M|      student|01002|      student|  196|\n",
      "| 10| 53|     M|       lawyer|90703|       lawyer|   12|\n",
      "| 13| 47|     M|     educator|29206|     educator|   95|\n",
      "| 14| 45|     M|    scientist|55106|    scientist|   31|\n",
      "+---+---+------+-------------+-----+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use SQL to query\n",
    "df.createOrReplaceTempView('users')\n",
    "results.createOrReplaceTempView('occupation_count')\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM users u\n",
    "JOIN occupation_count oc ON oc.occupation = u.occupation\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
